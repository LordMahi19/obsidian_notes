# Thesis Abstract

Event-based vision offers significant advantages for action recognition, particularly in dynamic, low-power applications. However, models trained on clean, laboratory-recorded datasets often fail to generalize to real-world scenarios characterized by diverse lighting, cluttered backgrounds, and varied viewpoints. This study investigates the robustness of a pre-trained 3D Convolutional Neural Network (3D-CNN) for event-based gesture recognition and quantifies the impact of data augmentation on its performance.

The core methodology involves three stages. First, a baseline performance is established by evaluating a 3D-CNN model, pre-trained on the DVS128 Gesture dataset, achieving an accuracy of approximately 84%. Second, a custom dataset is created by recording RGB videos of the same gestures under challenging real-world conditions—including variations in angle, lighting, distance, and background—and converting them into synthetic event streams using the `v2e` tool.

The study then systematically measures the performance degradation when the baseline model is tested against this new, more complex data. Subsequently, the model is re-trained on an augmented dataset that combines the original DVS128 training data with the newly generated real-world samples. By comparing the performance of the original model, the degraded model, and the re-trained model on the same augmented test set, this research aims to provide a quantitative answer to the extent to which real-world data augmentation can mitigate performance loss and improve the model's generalization. The findings are expected to demonstrate a practical and effective method for enhancing the robustness of event-based systems, making them more viable for deployment in uncontrolled environments.
